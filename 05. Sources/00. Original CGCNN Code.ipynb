{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"01. Original CGCNN Code.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyNFWEX2kEHJFTLgHPJMibcP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Original CGCNN (Crystal Graph Convolutional Neural Network) Code in github\n","\n","* Here, I just rewrite the original CGCNN code in Github repository and add some useful comment and modify some of the code.\n","\n"],"metadata":{"id":"SeDA4w-PeXNN"}},{"cell_type":"markdown","source":["## Google Drive Mount\n","\n"],"metadata":{"id":"iyFhuB4Wfx-L"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RkBUCnPCf4rm","executionInfo":{"status":"ok","timestamp":1642148931725,"user_tz":-540,"elapsed":20661,"user":{"displayName":"Hyeongseon Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14179648673617368098"}},"outputId":"0e31fddd-38c2-4cb3-f082-8da7a56f7076"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["cd /content/drive/MyDrive/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JWk1HTkCf4t6","executionInfo":{"status":"ok","timestamp":1642148931726,"user_tz":-540,"elapsed":12,"user":{"displayName":"Hyeongseon Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14179648673617368098"}},"outputId":"23657743-8c0b-48a3-a471-c80653289df0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive\n"]}]},{"cell_type":"code","source":["ls -l"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"23WX55RMf4wE","executionInfo":{"status":"ok","timestamp":1642148931726,"user_tz":-540,"elapsed":7,"user":{"displayName":"Hyeongseon Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14179648673617368098"}},"outputId":"e7bdbb86-9d67-4e71-da0a-69ff1d8e855a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["total 44\n","drwx------ 2 root root  4096 Oct 14  2020 \u001b[0m\u001b[01;34m'00. Github'\u001b[0m/\n","drwx------ 2 root root  4096 Nov  6  2020 \u001b[01;34m'01. Backup Files'\u001b[0m/\n","drwx------ 2 root root  4096 Jan 20  2021  \u001b[01;34mcgcnn\u001b[0m/\n","drwx------ 2 root root  4096 Oct  1 14:07  \u001b[01;34mTemp\u001b[0m/\n","-rw------- 1 root root 27670 Jan 13 06:52  temp.ipynb\n"]}]},{"cell_type":"markdown","source":["## CGCNN Github Repository Clone\n","\n"],"metadata":{"id":"teCKZQhMgMCq"}},{"cell_type":"code","source":["#!git clone https://github.com/txie-93/cgcnn"],"metadata":{"id":"-Xpc09PvgQub"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Pymatgen Library Installation\n","\n"],"metadata":{"id":"XLCpmuA5ftAR"}},{"cell_type":"code","source":["!pip install pymatgen==2020.11.11"],"metadata":{"id":"e0yHlN2HeXPo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip list | grep imgaug\n","!pip list | grep pymatgen\n","!python --version"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6b_GaFOkaNS0","executionInfo":{"status":"ok","timestamp":1642149020993,"user_tz":-540,"elapsed":1785,"user":{"displayName":"Hyeongseon Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14179648673617368098"}},"outputId":"b3c85316-857c-4e74-e463-994b48e11354"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["imgaug                        0.2.7\n","pymatgen                      2020.11.11\n","Python 3.7.12\n"]}]},{"cell_type":"markdown","source":["## **cgcnn/cgcnn/data.py**\n","\n"],"metadata":{"id":"PzTo508SeXWk"}},{"cell_type":"code","source":["from __future__ import print_function, division\n","\n","import csv\n","import functools\n","import json\n","import os\n","import random\n","import warnings\n","\n","import numpy as np\n","import torch\n","\n","from pymatgen.core.structure import Structure\n","from torch.utils.data import Dataset, DataLoader\n","from torch.utils.data.dataloader import default_collate\n","from torch.utils.data.sampler import SubsetRandomSampler"],"metadata":{"id":"cRe9aMmmeXY9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### get_train_val_test_loader\n","\n"],"metadata":{"id":"fTYkID4I4TyN"}},{"cell_type":"code","source":["def get_train_val_test_loader(dataset, collate_fn = default_collate,\n","                              batch_size = 64, train_ratio = None,\n","                              val_ratio = 0.1, test_ratio = 0.1, return_test = False,\n","                              num_workers = 1, pin_memory = False, **kwargs):\n","    \"\"\"\n","    Utility function for dividing a dataset to train, val, test datasets.\n","\n","    !!! The dataset needs to be shuffled before using the function !!!\n","\n","    Parameters\n","    ----------\n","    dataset : torch.utils.data.Dataset\n","              The full dataset to be divided.\n","    collate_fn : torch.utils.data.DataLoader\n","    batch_size : int\n","    train_ratio : float\n","    val_ratio : float\n","    test_ratio : float\n","    return_test : bool\n","                  Whether to return the test dataset loader. \n","                  If False, the last test_size data will be hidden.\n","    num_workers : int\n","    pin_memory : bool\n","\n","    Returns\n","    -------\n","    train_loader : torch.utils.data.DataLoader\n","                   DataLoader that random samples the training data.\n","    val_loader : torch.utils.data.DataLoader\n","                 DataLoader that random samples the validation data.\n","    (test_loader) : torch.utils.data.DataLoader\n","                    DataLoader that random samples the test data, returns if return_test = True.\n","    \"\"\"\n","    total_size = len(dataset) # Number of total data\n","\n","    if kwargs['train_size'] is None:\n","        if train_ratio is None:\n","            assert val_ratio + test_ratio < 1\n","            train_ratio = 1 - val_ratio - test_ratio\n","            print('[Warning] train_ratio is None, using 1 - val_ratio - test_ratio = {} as training data.'.format(train_ratio))\n","        else:\n","            assert train_ratio + val_ratio + test_ratio <= 1\n","\n","    indices = list(range(total_size)) # Index list of total data\n","\n","    if kwargs['train_size']:\n","        train_size = kwargs['train_size']\n","    else:\n","        train_size = int(train_ratio * total_size)\n","\n","    if kwargs['test_size']:\n","        test_size = kwargs['test_size']\n","    else:\n","        test_size = int(test_ratio * total_size)\n","\n","    if kwargs['val_size']:\n","        valid_size = kwargs['val_size']\n","    else:\n","        valid_size = int(val_ratio * total_size)\n","\n","    # Random sampler for training, validation dataset\n","    train_sampler = SubsetRandomSampler(indices = indices[:train_size]) \n","    val_sampler   = SubsetRandomSampler(indices = indices[ -(valid_size + test_size) : -test_size ])\n","\n","    if return_test: # Test data loader를 사용하는 경우\n","        test_sampler = SubsetRandomSampler(indices = indices[-test_size:])\n","\n","    train_loader = DataLoader(dataset, batch_size = batch_size,\n","                              sampler = train_sampler,\n","                              num_workers = num_workers,\n","                              collate_fn = collate_fn, pin_memory = pin_memory)\n","    val_loader = DataLoader(dataset, batch_size = batch_size,\n","                            sampler = val_sampler,\n","                            num_workers = num_workers,\n","                            collate_fn = collate_fn, pin_memory = pin_memory)\n","    \n","    if return_test: # Test data loader를 사용하는 경우\n","        test_loader = DataLoader(dataset, batch_size = batch_size,\n","                                 sampler = test_sampler,\n","                                 num_workers = num_workers,\n","                                 collate_fn = collate_fn, pin_memory = pin_memory)  \n","        \n","    if return_test: # Test data loader를 사용하는 경우\n","        return train_loader, val_loader, test_loader\n","    else:\n","        return train_loader, val_loader"],"metadata":{"id":"raGk7-1weXa-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### collate_pool\n","\n"],"metadata":{"id":"5nx7Ykvh4QRb"}},{"cell_type":"code","source":["def collate_pool(dataset_list):\n","    \"\"\"\n","    Collate a list of data and return a batch for predicting crystal properties.\n","    (Crystal Graph data에 대한 Graph batch를 생성하는 함수)\n","\n","    Parameters\n","    ----------\n","\n","    dataset_list : list of tuples for each data point.\n","                   (atom_fea, nbr_fea, nbr_fea_idx, target)\n","                   \n","                   atom_fea : torch.Tensor shape (n_i, atom_fea_len)\n","                   nbr_fea : torch.Tensor shape (n_i, M, nbr_fea_len)\n","                   nbr_fea_idx : torch.LongTensor shape (n_i, M)\n","                   target : torch.Tensor shape (1, )\n","                   cif_id : str or int\n","\n","    Returns\n","    -------\n","    N = sum(n_i); N0 = sum(i)\n","\n","    batch_atom_fea : torch.Tensor shape (N, orig_atom_fea_len)\n","                     Atom features from atom type\n","    batch_nbr_fea : torch.Tensor shape (N, M, nbr_fea_len)\n","                    Bond features of each atom's M neighbors\n","    batch_nbr_fea_idx : torch.LongTensor shape (N, M)\n","                        Indices of M neighbors of each atom\n","    crystal_atom_idx : list of torch.LongTensor of length N0\n","                       Mapping from the crystal idx to atom idx\n","    target : torch.Tensor shape (N, 1)\n","             Target value for prediction\n","    batch_cif_ids : list\n","    \"\"\"\n","    batch_atom_fea, batch_nbr_fea, batch_nbr_fea_idx = [], [], []\n","    crystal_atom_idx, batch_target = [], []\n","    batch_cif_ids = []\n","    base_idx = 0\n","\n","    for i, ((atom_fea, nbr_fea, nbr_fea_idx), target, cif_id) in enumerate(dataset_list):\n","\n","        n_i = atom_fea.shape[0]  # Number of atoms for this crystal\n","        batch_atom_fea.append(atom_fea)\n","        batch_nbr_fea.append(nbr_fea)\n","        batch_nbr_fea_idx.append(nbr_fea_idx + base_idx)\n","\n","        new_idx = torch.LongTensor(np.arange(n_i) + base_idx)\n","\n","        crystal_atom_idx.append(new_idx)\n","        batch_target.append(target)\n","        batch_cif_ids.append(cif_id)\n","        base_idx += n_i\n","\n","    return (torch.cat(batch_atom_fea, dim = 0),\n","            torch.cat(batch_nbr_fea, dim = 0),\n","            torch.cat(batch_nbr_fea_idx, dim = 0),\n","            crystal_atom_idx), \\\n","            torch.stack(batch_target, dim = 0), \\\n","            batch_cif_ids"],"metadata":{"id":"jzHOF_nNoZU2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### GaussianDistance\n","\n"],"metadata":{"id":"pPP7AcTc4M8P"}},{"cell_type":"code","source":["class GaussianDistance(object):\n","    \"\"\"\n","    Expands the distance by Gaussian basis. (원자 사이의 거리(distance)를 Gaussian basis로 확장하는 클래스)\n","\n","    Unit : Angstrom\n","    \"\"\"\n","    def __init__(self, dmin, dmax, step, var = None):\n","        \"\"\"\n","        Parameters\n","        ----------\n","\n","        dmin : float\n","               Minimum interatomic distance\n","        dmax : float\n","               Maximum interatomic distance\n","        step : float\n","               Step size for the Gaussian filter\n","        \"\"\"\n","        assert dmin < dmax\n","        assert dmax - dmin > step\n","        self.filter = np.arange(dmin, dmax + step, step)\n","        if var is None:\n","            var = step\n","        self.var = var \n","\n","    def expand(self, distances):\n","        \"\"\"\n","        Apply Gaussian distance filter to a numpy distance array\n","\n","        Parameters\n","        ----------\n","\n","        distance : np.array shape n-d array\n","                   A distance matrix of any shape\n","        \n","        Returns\n","        -------\n","        expanded_distance : shape (n+1)-d array\n","                            Expanded distance matrix with the last dimension of length len(self.filter)\n","        \"\"\"\n","        return np.exp( -(distances[..., np.newaxis] - self.filter)**2 / self.var**2 )"],"metadata":{"id":"iYGQZs6ZoZZ4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### AtomInitializer\n","\n"],"metadata":{"id":"ck-Wo3lj4KCF"}},{"cell_type":"code","source":["class AtomInitializer(object):\n","    \"\"\"\n","    Base class for initializing the vector representation for atoms.\n","\n","    !!! Use one AtomInitializer per dataset !!!\n","    \"\"\"\n","    def __init__(self, atom_types):\n","        self.atom_types = set(atom_types) # set(집합)으로 중복된 atom type 있으면 제거\n","        self._embedding = {}\n","\n","    def get_atom_fea(self, atom_type):    # atom type에 따른 initial vector representation을 return하는 method\n","        assert atom_type in self.atom_types \n","        return self._embedding[atom_type]\n","\n","    def load_state_dict(self, state_dict):\n","        self._embedding = state_dict\n","        self.atom_types = set(self._embedding.keys())\n","        self._decodedict = {idx : atom_type for atom_type, idx in self._embedding.items()}\n","\n","    def state_dict(self):\n","        return self._embedding\n","\n","    def decode(self, idx):\n","        if not hasattr(self, '_decodedict'):\n","            self._decodedict = {idx : atom_type for atom_type, idx in self._embedding.items()}\n","        return self._decodedict[idx]"],"metadata":{"id":"HsuzW9COoZcY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### AtomCustomJSONInitializer\n","\n"],"metadata":{"id":"ZuBO_GBa4F4X"}},{"cell_type":"code","source":["class AtomCustomJSONInitializer(AtomInitializer):\n","    \"\"\"\n","    Initialize atom feature vectors using a JSON file, which is a python\n","    dictionary mapping from element number to a list representing the\n","    feature vector of the element.\n","\n","    Parameters\n","    ----------\n","\n","    elem_embedding_file : str\n","                          The path to the .json file\n","    \"\"\"\n","    def __init__(self, elem_embedding_file):\n","        with open(elem_embedding_file) as f: # JSON file open\n","            elem_embedding = json.load(f)    # Load JSON file contents\n","        elem_embedding = {int(key) : value for key, value in elem_embedding.items()} # string key(atomic number) -> integer\n","        atom_types = set(elem_embedding.keys()) # set(집합)으로 중복된 atom number 있으면 제거\n","        super(AtomCustomJSONInitializer, self).__init__(atom_types)\n","        for key, value in elem_embedding.items():\n","            self._embedding[key] = np.array(value, dtype = float) # atomic number에 따른 value를 np.array로 embedding 속성에 저장"],"metadata":{"id":"FRZYOqvPoTXa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### CIFData\n","\n"],"metadata":{"id":"DPVgqb2S4C5R"}},{"cell_type":"code","source":["class CIFData(Dataset):\n","    \"\"\"\n","    The CIFData dataset is a wrapper for a dataset where the crystal structures\n","    are stored in the form of CIF files. The dataset should have the following \n","    directory structure:\n","\n","    root_dir\n","    ├── id_prop.csv\n","    ├── atom_init.json\n","    ├── id0.cif\n","    ├── id1.cif\n","    ├── ...\n","\n","    id_prop.csv : a CSV file with two columns. The first column recodes a \n","    unique ID for each crystal, and the second column recodes the value of\n","    target property.\n","\n","    atom_init.json : a JSON file that stores the initialization vector for each element.\n","\n","    ID.cif : a CIF file that recodes the crystal structure, where ID is the\n","    unique ID for the crystal.\n","\n","    Parameters\n","    ----------\n","\n","    root_dir : str\n","               The path to the root directory of the dataset\n","    max_num_nbr : int\n","                  The maximum number of neighbors while constructing the crystal graph\n","    radius : float\n","             The cutoff radius for searching neighbors\n","    dmin : float\n","           The minimum distance for constructing GaussianDistance\n","    step : float\n","           The step size for constructing GaussianDistance\n","    random_seed : int\n","                  Random seed for shuffling the dataset\n","\n","    Returns\n","    -------\n","\n","    atom_fea : torch.Tensor shape (n_i, atom_fea_len)\n","    nbr_fea : torch.Tensor shape (n_i, M, nbr_fea_len)\n","    nbr_fea_idx : torch.LongTensor shape (n_i, M)\n","    target : torch.Tensor shape (1, )\n","    cif_id : str or int\n","    \"\"\"\n","    def __init__(self, root_dir, max_num_nbr = 12, radius = 8, dmin = 0, step = 0.2,\n","                 random_seed = 123):\n","        self.root_dir = root_dir\n","        self.max_num_nbr, self.radius = max_num_nbr, radius\n","        assert os.path.exists(root_dir), 'root_dir does not exist!'\n","\n","        id_prop_file = os.path.join(self.root_dir, 'id_prop.csv')\n","        assert os.path.exists(id_prop_file), 'id_prop.csv does not exist!'\n","\n","        with open(id_prop_file) as f:\n","            reader = csv.reader(f)\n","            self.id_prop_data = [row for row in reader]\n","        \n","        random.seed(random_seed)\n","        random.shuffle(self.id_prop_data)\n","\n","        atom_init_file = os.path.join(self.root_dir, 'atom_init.json')\n","        assert os.path.exists(atom_init_file), 'atom_init.json file does not exist!'\n","\n","        self.ari = AtomCustomJSONInitializer(atom_init_file)\n","        self.gdf = GaussianDistance(dmin = dmin, dmax = self.radius, step = step)\n","\n","    def __len__(self):\n","        return len(self.id_prop_data)\n","\n","    @functools.lru_cache(maxsize = None)  # Cache loaded structures\n","    def __getitem__(self, idx):\n","        cif_id, target = self.id_prop_data[idx]\n","        crystal = Structure.from_file(os.path.join(self.root_dir, cif_id + '.cif'))\n","\n","        atom_fea = np.vstack([self.ari.get_atom_fea(crystal[i].specie.number) for i in range(len(crystal))])\n","        atom_fea = torch.Tensor(atom_fea)\n","\n","        all_nbrs = crystal.get_all_neighbors(self.radius, include_index = True)\n","        all_nbrs = [sorted(nbrs, key = lambda x : x[1]) for nbrs in all_nbrs]\n","\n","        nbr_fea_idx, nbr_fea = [], []\n","        for nbr in all_nbrs:\n","            if len(nbr) < self.max_num_nbr:\n","                warnings.warn('{} not find enough neighbors to build graph. If it happens frequently, consider increase radius.'.format(cif_id))\n","                nbr_fea_idx.append( list(map(lambda x : x[2], nbr)) + [0] * (self.max_num_nbr - len(nbr)) )\n","                nbr_fea.append( list(map(lambda x : x[1], nbr)) + [self.radius + 1.] * (self.max_num_nbr - len(nbr)) )\n","            else:\n","                nbr_fea_idx.append( list(map(lambda x : x[2], nbr[:self.max_num_nbr])) )\n","                nbr_fea.append( list(map(lambda x : x[1], nbr[:self.max_num_nbr])) )\n","        \n","        nbr_fea_idx, nbr_fea = np.array(nbr_fea_idx), np.array(nbr_fea)\n","        nbr_fea = self.gdf.expand(nbr_fea)\n","        atom_fea = torch.Tensor(atom_fea)\n","        nbr_fea = torch.Tensor(nbr_fea)\n","        nbr_fea_idx = torch.LongTensor(nbr_fea_idx)\n","        target = torch.Tensor([float(target)])\n","\n","        return (atom_fea, nbr_fea, nbr_fea_idx), target, cif_id\n"],"metadata":{"id":"lYoN6BAZ2J3G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **cgcnn/cgcnn/model.py**\n","\n"],"metadata":{"id":"t0Bdsj2FFpj9"}},{"cell_type":"code","source":["from __future__ import print_function, division\n","\n","import torch\n","import torch.nn as nn"],"metadata":{"id":"IiENztuUFpnC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ConvLayer\n","\n"],"metadata":{"id":"Zt0tFhya3_qZ"}},{"cell_type":"code","source":["class ConvLayer(nn.Module):\n","    \"\"\"\n","    Convolutional operation on graphs\n","    \"\"\"\n","    def __init__(self, atom_fea_len, nbr_fea_len):\n","        \"\"\"\n","        Initialize ConvLayer.\n","\n","        Parameters\n","        ----------\n","\n","        atom_fea_len : int\n","                       Number of atom hidden features.\n","        nbr_fea_len : int\n","                      Number of bond features.\n","        \"\"\"\n","        super(ConvLayer, self).__init__()\n","        self.atom_fea_len = atom_fea_len\n","        self.nbr_fea_len = nbr_fea_len\n","        self.fc_full = nn.Linear(in_features = 2 * self.atom_fea_len + self.nbr_fea_len,\n","                                 out_features = 2 * self.atom_fea_len)\n","        self.sigmoid = nn.Sigmoid()\n","        self.softplus1 = nn.Softplus()\n","        self.bn1 = nn.BatchNorm1d(num_features = 2 * self.atom_fea_len)\n","        self.bn2 = nn.BatchNorm1d(num_features = self.atom_fea_len)\n","        self.softplus2 = nn.Softplus()\n","\n","    def forward(self, atom_in_fea, nbr_fea, nbr_fea_idx):\n","        \"\"\"\n","        Forward pass\n","\n","        N : Total number of atoms in the batch\n","        M : Max number of neighbors\n","\n","        Parameters\n","        ----------\n","\n","        atom_in_fea : Variable(torch.Tensor) shape (N, atom_fea_len)\n","                      Atom hidden features before convolution\n","        nbr_fea : Variable(torch.Tensor) shape (N, M, nbr_fea_len)\n","                  Bond features of each atom's M neighbors\n","        nbr_fea_idx : torch.LongTensor shape (N, M)\n","                      Indices of M neighbors of each atom\n","\n","        Returns\n","        -------\n","\n","        atom_out_fea : nn.Variable shape (N, atom_fea_len)\n","                       Atom hidden features after convolution\n","        \"\"\"\n","        # TODO will there be problems with the index zero padding?\n","        N, M = nbr_fea_idx.shape\n","\n","        # Convolution -> this process should be investigated with exact shape per each step!!\n","        # We can modify this process\n","        atom_nbr_fea = atom_in_fea[nbr_fea_idx, :] # important operation!\n","        total_nbr_fea = torch.cat([atom_in_fea.unsqueeze(1).expand(N, M, self.atom_fea_len), atom_nbr_fea, nbr_fea], dim = 2)\n","\n","        total_gated_fea = self.fc_full(total_nbr_fea)\n","        total_gated_fea = self.bn1(total_gated_fea.view(-1, self.atom_fea_len * 2)).view(N, M, self.atom_fea_len * 2)\n","\n","        nbr_filter, nbr_core = total_gated_fea.chunk(2, dim = 2)\n","        nbr_filter = self.sigmoid(nbr_filter)\n","        nbr_core = self.softplus1(nbr_core)\n","\n","        nbr_sumed = torch.sum(nbr_filter * nbr_core, dim = 1)\n","        nbr_sumed = self.bn2(nbr_sumed)\n","        \n","        out = self.softplus2(atom_in_fea + nbr_sumed)\n","        return out\n"],"metadata":{"id":"BSMIslIBFppm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### CrystalGraphConvNet\n","\n"],"metadata":{"id":"N_0kQupR37sQ"}},{"cell_type":"code","source":["class CrystalGraphConvNet(nn.Module):\n","    \"\"\"\n","    Create a crystal graph convolutional neural network for predicting total\n","    material properties.\n","    \"\"\"\n","    def __init__(self, orig_atom_fea_len, nbr_fea_len,\n","                 atom_fea_len = 64, n_conv = 3, h_fea_len = 128, n_h = 1,\n","                 classification = False):\n","        \"\"\"\n","        Initialize CrystalGraphConvNet.\n","\n","        Parameters\n","        ----------\n","\n","        orig_atom_fea_len : int\n","                            Number of atom features in the input.\n","        nbr_fea_len : int\n","                      Number of bond features.\n","        atom_fea_len : int\n","                       Number of hidden atom features in the convolutional layers\n","        n_conv : int\n","                 Number of convolutional layers\n","        h_fea_len : int\n","                    Number of hidden features after pooling\n","        n_h : int\n","              Number of hidden layers after pooling\n","        \"\"\"\n","        super(CrystalGraphConvNet, self).__init__()\n","        self.classification = classification\n","        self.embedding = nn.Linear(in_features = orig_atom_fea_len, out_features = atom_fea_len) # Dimension reduction layer\n","        self.convs = nn.ModuleList([ConvLayer(atom_fea_len = atom_fea_len,\n","                                              nbr_fea_len = nbr_fea_len) for _ in range(n_conv)]) # Iterative Graph Convolution layers\n","        self.conv_to_fc = nn.Linear(atom_fea_len, h_fea_len)\n","        self.conv_to_fc_softplus = nn.Softplus()\n","\n","        if n_h > 1: # Iterative FCL layers\n","            self.fcs = nn.ModuleList([nn.Linear(h_fea_len, h_fea_len) for _ in range(n_h - 1)])\n","            self.softpluses = nn.ModuleList([nn.Softplus() for _ in range(n_h - 1)])\n","        \n","        if self.classification:\n","            self.fc_out = nn.Linear(h_fea_len, 2) # Binary classification\n","        else:\n","            self.fc_out = nn.Linear(h_fea_len, 1) # Only one physical property Regression -> Maybe we can modify into multiple property prediction\n","        \n","        if self.classification:\n","            self.logsoftmax = nn.LogSoftmax(dim = 1)\n","            self.dropout = nn.Dropout()\n","\n","    def forward(self, atom_fea, nbr_fea, nbr_fea_idx, crystal_atom_idx):\n","        \"\"\"\n","        Forward pass\n","\n","        N : Total number of atoms in the batch\n","        M : Max number of neighbors\n","        N0 : Total number of crystals in the batch\n","\n","        Parameters\n","        ----------\n","\n","        atom_fea : Variable(torch.Tensor) shape (N, orig_atom_fea_len)\n","                   Atom features from atom type\n","        nbr_fea : Variable(torch.Tensor) shape (N, M, nbr_fea_len)\n","                  Bond features of each atom's M neighbors\n","        nbr_fea_idx : torch.LongTensor shape (N, M)\n","                      Indices of M neighbors of each atom\n","        crystal_atom_idx : list of torch.LongTensor of length N0\n","                           Mapping from the crystal idx to atom idx\n","\n","        Returns\n","        -------\n","\n","        prediction : nn.Variable shape (N, )\n","                     Atom hidden features after convolution\n","        \"\"\"\n","        atom_fea = self.embedding(atom_fea)\n","\n","        for conv_func in self.convs: # Iterative Graph Convolution\n","            atom_fea = conv_func(atom_fea, nbr_fea, nbr_fea_idx)\n","        \n","        crys_fea = self.pooling(atom_fea, crystal_atom_idx) # Graph Pooling\n","        crys_fea = self.conv_to_fc(self.conv_to_fc_softplus(crys_fea))\n","        crys_fea = self.conv_to_fc_softplus(crys_fea)\n","\n","        if self.classification:\n","            crys_fea = self.dropout(crys_fea)\n","\n","        if hasattr(self, 'fcs') and hasattr(self, 'softpluses'): # Check whether the instance have 'fcs' and 'softpluses' attributes\n","            for fc, softplus in zip(self.fcs, self.softpluses):\n","                crys_fea = softplus(fc(crys_fea))\n","        \n","        out = self.fc_out(crys_fea)\n","\n","        if self.classification:\n","            out = self.logsoftmax(out)\n","        \n","        return out\n","\n","    def pooling(self, atom_fea, crystal_atom_idx):\n","        \"\"\"\n","        Pooling the atom features to crystal features\n","\n","        N : Total number of atoms in the batch\n","        N0 : Total number of crystals in the batch\n","\n","        Parameters\n","        ----------\n","\n","        atom_fea : Variable(torch.Tensor) shape (N, atom_fea_len)\n","                   Atom feature vectors of the batch\n","        crystal_atom_idx : list of torch.LongTensor of length N0\n","                           Mapping from the crystal idx to atom idx\n","        \"\"\"\n","        assert sum([len(idx_map) for idx_map in crystal_atom_idx]) == atom_fea.data.shape[0]\n","\n","        # Simple Mean Pooling operation --> We can modify it to the modern pooling function\n","        summed_fea = [torch.mean(atom_fea[idx_map], dim = 0, keepdim = True) for idx_map in crystal_atom_idx]\n","\n","        return torch.cat(summed_fea, dim = 0)\n"],"metadata":{"id":"gT9oRak-Fpwc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **cgcnn/main.py**\n","\n"],"metadata":{"id":"OngqXFlO0yZ8"}},{"cell_type":"code","source":["import argparse\n","import os\n","import shutil\n","import sys\n","import time\n","import warnings\n","from random import sample \n","\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","from sklearn import metrics\n","from torch.autograd import Variable\n","from torch.optim.lr_scheduler import MultiStepLR\n","\n","# from cgcnn.data import CIFData\n","# from cgcnn.data import collate_pool, get_train_val_test_loader\n","# from cgcnn.model import CrystalGraphConvNet"],"metadata":{"id":"qXu3Byp90yc4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Args"],"metadata":{"id":"vVfOQkXz3VOe"}},{"cell_type":"code","source":["parser = argparse.ArgumentParser(description = 'Crystal Graph Convolutional Neural Networks')\n","\n","parser.add_argument('data_options', metavar = 'OPTIONS', nargs = '+',\n","                    help = 'dataset options, started with the path to root dir, then other options')\n","parser.add_argument('--task', choices = ['regression', 'classification'],\n","                    default = 'regression', help = 'complete a regression or classification task (default : regression)')\n","parser.add_argument('--disable-cuda', action = 'store_true',\n","                    help = 'Disable CUDA')\n","parser.add_argument('-j', '--workers', default = 0, type = int, metavar = 'N',\n","                    help = 'number of data loading workers (default : 0)')\n","parser.add_argument('--epochs', default = 30, type = int, metavar = 'N',\n","                    help = 'number of total epochs to run (default : 30)')\n","parser.add_argument('--start-epoch', default = 0, type = int, metavar = 'N',\n","                    help = 'manual epoch number (useful on restarts)')\n","parser.add_argument('-b', '--batch-size', default = 256, type = int,\n","                    metavar = 'N', help = 'mini-batch size (default : 256)')\n","parser.add_argument('--lr', '--learning-rate', default = 0.01, type = float,\n","                    metavar = 'LR', help = 'initial learning rate (default : 0.01)')\n","parser.add_argument('--lr-milestones', default = [100], nargs = '+', type = int,\n","                    metavar = 'N', help = 'milestones for scheduler (default : [100])')\n","parser.add_argument('--momentum', default = 0.9, type = float, metavar = 'M',\n","                    help = 'momentum')\n","parser.add_argument('--weight-decay', '--wd', default = 0, type = float,\n","                    metavar = 'W', help = 'weight decay (default : 0)')\n","parser.add_argument('--print-freq', '-p', default = 10, type = int,\n","                    metavar = 'N', help = 'print frequency (default : 10)')\n","parser.add_argument('--resume', default = '', type = str, metavar = 'PATH',\n","                    help = 'path to latest checkpoint (default : none)')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2m4MxFEY0yjQ","executionInfo":{"status":"ok","timestamp":1642149048365,"user_tz":-540,"elapsed":56,"user":{"displayName":"Hyeongseon Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14179648673617368098"}},"outputId":"523f05fc-96f9-4276-bcc3-a772134a922f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["_StoreAction(option_strings=['--resume'], dest='resume', nargs=None, const=None, default='', type=<class 'str'>, choices=None, help='path to latest checkpoint (default : none)', metavar='PATH')"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["train_group = parser.add_mutually_exclusive_group()\n","\n","train_group.add_argument('--train-ratio', default = None, type = float, metavar = 'N',\n","                         help = 'percentage of training data to be loaded (default : none)')\n","train_group.add_argument('--train-size', default = None, type = int, metavar = 'N',\n","                         help = 'number of training data to be loaded (default : none)')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LzJOvZwk0ymU","executionInfo":{"status":"ok","timestamp":1642149049215,"user_tz":-540,"elapsed":4,"user":{"displayName":"Hyeongseon Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14179648673617368098"}},"outputId":"365b9f49-52e8-4721-9d97-3a033a5d9d21"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["_StoreAction(option_strings=['--train-size'], dest='train_size', nargs=None, const=None, default=None, type=<class 'int'>, choices=None, help='number of training data to be loaded (default : none)', metavar='N')"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["valid_group = parser.add_mutually_exclusive_group()\n","\n","valid_group.add_argument('--val-ratio', default = 0.1, type = float, metavar = 'N',\n","                         help = 'percentage of validation data to be loaded (default : 0.1)')\n","valid_group.add_argument('--val-size', default = 1000, type = int, metavar = 'N',\n","                         help = 'number of validation data to be loaded (default : 1000)')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XIZoYeMv0ypk","executionInfo":{"status":"ok","timestamp":1642149049623,"user_tz":-540,"elapsed":3,"user":{"displayName":"Hyeongseon Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14179648673617368098"}},"outputId":"edca3bb7-ec0a-4b37-8455-e7fb54f6090c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["_StoreAction(option_strings=['--val-size'], dest='val_size', nargs=None, const=None, default=None, type=<class 'int'>, choices=None, help='number of validation data to be loaded (default : 1000)', metavar='N')"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["test_group = parser.add_mutually_exclusive_group()\n","\n","test_group.add_argument('--test-ratio', default = 0.1, type = float, metavar = 'N',\n","                        help = 'percentage of test data to be loaded (default : 0.1)')\n","test_group.add_argument('--test-size', default = 1000, type = int, metavar = 'N',\n","                        help = 'number of test data to be loaded (default : 1000)')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zMH7E56I0ysU","executionInfo":{"status":"ok","timestamp":1642149050456,"user_tz":-540,"elapsed":3,"user":{"displayName":"Hyeongseon Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14179648673617368098"}},"outputId":"dcd9e187-ee3c-42a3-b5a1-4817998e980d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["_StoreAction(option_strings=['--test-size'], dest='test_size', nargs=None, const=None, default=None, type=<class 'int'>, choices=None, help='number of test data to be loaded (default : 1000)', metavar='N')"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["parser.add_argument('--optim', default = 'SGD', type = str, metavar = 'SGD',\n","                    help = 'choose an optimizer, SGD or Adam, (default : SGD)')\n","parser.add_argument('--atom-fea-len', default = 64, type = int, metavar = 'N',\n","                    help = 'number of hidden atom features in conv layers')\n","parser.add_argument('--h-fea-len', default = 128, type = int, metavar = 'N',\n","                    help = 'number of hidden features after pooling')\n","parser.add_argument('--n-conv', default = 3, type = int, metavar = 'N',\n","                    help = 'number of conv layers')\n","parser.add_argument('--n-h', default = 1, type = int, metavar = 'N',\n","                    help = 'number of hidden layers after pooling')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3r4DKU6T0yvK","executionInfo":{"status":"ok","timestamp":1642149051517,"user_tz":-540,"elapsed":58,"user":{"displayName":"Hyeongseon Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14179648673617368098"}},"outputId":"b00f5394-3b10-40a7-8c97-aa7165e88b4d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["_StoreAction(option_strings=['--n-h'], dest='n_h', nargs=None, const=None, default=1, type=<class 'int'>, choices=None, help='number of hidden layers after pooling', metavar='N')"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["args = parser.parse_args(sys.argv[1:])\n","args.cuda = not args.disable_cuda and torch.cuda.is_available()"],"metadata":{"id":"t4SEPpEs0yxm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if args.task == 'regression':\n","    best_mae_error = 1e10\n","else:\n","    best_mae_error = 0.\n","\n","print(args.task, best_mae_error)"],"metadata":{"id":"P3S5qQJY2KH4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Normalizer\n","\n"],"metadata":{"id":"zNpYNJx39tt5"}},{"cell_type":"code","source":["class Normalizer(object): \n","    \"\"\" Normalize a Tensor and restore it later. \"\"\"\n","    \"\"\" 주어진 Tensor data의 평균과 표준편차를 계산하고 이를 기반으로 정규화하는 클래스 \"\"\"\n","    \n","    def __init__(self, tensor):\n","        \"\"\" tensor is taken as a sample to calculate the mean and std \"\"\"\n","        self.mean = torch.mean(tensor)\n","        self.std  = torch.std(tensor)\n","\n","    def norm(self, tensor): # Normalize method\n","        return (tensor - self.mean) / self.std\n","\n","    def denorm(self, normed_tensor): # Denormalize method\n","        return normed_tensor * self.std + self.mean\n","\n","    def state_dict(self):\n","        return {'mean' : self.mean, 'std' : self.std}\n","    \n","    def load_state_dict(self, state_dict):\n","        self.mean = state_dict['mean']\n","        self.std  = state_dict['std']"],"metadata":{"id":"zm1jNb1M9twL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### mae\n","\n"],"metadata":{"id":"wX3kfpiN3ecQ"}},{"cell_type":"code","source":["def mae(prediction, target):\n","    \"\"\"\n","    Computes the mean absolute error between prediction and target\n","\n","    Parameters\n","    ----------\n","\n","    prediction : torch.Tensor (N, 1)\n","    target : torch.Tensor (N, 1)\n","    \"\"\"\n","    return torch.mean(torch.abs(target - prediction))"],"metadata":{"id":"HtIyBRMQ-gNy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### class_eval\n","\n"],"metadata":{"id":"87eGVXoM3ggx"}},{"cell_type":"code","source":["def class_eval(prediction, target):\n","    \"\"\" Class(Category) evaluation function only used for (binary -> ex) metal or semiconductor?) classification of the model \"\"\"\n","\n","    prediction = np.exp(prediction.numpy()) # Convert to probability\n","    target = target.numpy()\n","    pred_label = np.argmax(prediction, axis = 1) \n","    target_label = np.squeeze(target)\n","\n","    if not target_label.shape:\n","        target_label = np.asarray([target_label])\n","    \n","    if prediction.shape[1] == 2:\n","        precision, recall, fscore, _ = metrics.precision_recall_fscore_support(target_label,\n","                                                                               pred_label,\n","                                                                               average = 'binary') # binary classification\n","        auc_score = metrics.roc_auc_score(target_label, prediction[:, 1]) # AUC\n","        accuracy = metrics.accuracy_score(target_label, pred_label)       # Accuracy\n","    else:\n","        raise NotImplementedError\n","\n","    return accuracy, precision, recall, fscore, auc_score"],"metadata":{"id":"c6kuNE9XBKmm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### AverageMeter\n","\n"],"metadata":{"id":"L3mwwtIA3jJO"}},{"cell_type":"code","source":["class AverageMeter(object):\n","    \"\"\" Computes and stores the average and current value \"\"\"\n","    \"\"\" 기록하고 싶은 특정 지표들을 추적하는 클래스 \"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n = 1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count"],"metadata":{"id":"M8yYBLdnG-HF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### save_checkpoint\n","\n"],"metadata":{"id":"ZzO9TSFk3m8x"}},{"cell_type":"code","source":["def save_checkpoint(state, is_best, filename = 'checkpoint.pth.tar'):\n","    torch.save(state, filename) # 모델의 state dictionary를 file name으로 저장\n","    if is_best: # 최고성능 모델이면\n","        shutil.copyfile(filename, 'model_best.pth.tar') "],"metadata":{"id":"nmcZvLg6G-Lw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### adjust_learning_rate\n","\n"],"metadata":{"id":"gxhCGxVH3prk"}},{"cell_type":"code","source":["def adjust_learning_rate(optimizer, epoch, k):\n","    \"\"\" Sets the learning rate to the initial LR decayed by 10 every k epochs \"\"\"\n","    \"\"\" 학습률(Learning rate)의 decay를 조정하는 함수 \"\"\"\n","    assert type(k) is int\n","    lr = args.lr * (0.1 ** (epoch // k))\n","    for param_group in optimizer.param_groups:\n","        param_group['lr'] = lr"],"metadata":{"id":"wIo_BwxkG-PA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### train\n","\n"],"metadata":{"id":"CAVA0OzU3smh"}},{"cell_type":"code","source":["def train(train_loader, model, criterion, optimizer, epoch, normalizer):\n","\n","    # We will record batch_time, data_time, losses with value and average\n","    batch_time = AverageMeter() # batch process runtime\n","    data_time  = AverageMeter() # data loading time\n","    losses     = AverageMeter()\n","\n","    if args.task == 'regression':\n","        mae_errors = AverageMeter() # MAE : Mean Absolute Error, 평균절대오차\n","    else:          # classification\n","        accuracies = AverageMeter() # 정확도 : 직관적으로 모델성능을 나타내는 지표, 그러나 domain의 편중(bias)를 고려할 것!!\n","        precisions = AverageMeter() # 정밀도 : 모델이 True라고 분류한 것 중 실제 True인 것의 비율 (Positive 정답률, PPV(Positive Predictive Value))\n","        recalls    = AverageMeter() # 재현율 : 실제 True인 것 중에서 모델이 True라고 예측한 것의 비율 (통계학에서의 sensitivity, hit rate)\n","        fscores    = AverageMeter() # F1 score : 데이터 label이 불균형 구조일 때, 모델성능을 정확하게 평가하는 지표\n","        auc_scores = AverageMeter() # AUC(Area Under Curve) : ROC curve는 그래프이므로 명확한 수치표현 불가 -> 그래프 아래 면적값 (AUC) 이용, 최대값 1에 가까울수록 좋은 모델\n","\n","    # Switch to train mode\n","    model.train()\n","\n","    end = time.time()\n","    for i, (input, target, _) in enumerate(train_loader):\n","\n","        # Measure data loading time\n","        data_time.update(time.time() - end)\n","\n","        if args.cuda: # GPU -> 데이터를 variable로 만들고 모두 cuda device로 넘김.\n","            input_var = (Variable(input[0].cuda(non_blocking = True)),\n","                         Variable(input[1].cuda(non_blocking = True)),\n","                         input[2].cuda(non_blocking = True),\n","                         [crys_idx.cuda(non_blocking = True) for crys_idx in input[3]])\n","        else:         # CPU -> 데이터를 variable로 만들고, 그대로 CPU device에서 사용\n","            input_var = (Variable(input[0]),\n","                         Variable(input[1]),\n","                         input[2],\n","                         input[3])\n","        \n","        # Normalize target\n","        if args.task == 'regression':\n","            target_normed = normalizer.norm(target) # target value들을 normalize(정규화)\n","        else: # classification\n","            target_normed = target.view(-1).long()  # target(label)들을 1차원 벡터 및 정수데이터로 변환\n","\n","        if args.cuda: # GPU\n","            target_var = Variable(target_normed.cuda(non_blocking = True)) # target value -> cuda device로 넘김.\n","        else: # CPU\n","            target_var = Variable(target_normed) # target value -> 그대로 CPU device에서 사용\n","\n","        # Compute output\n","        output = model(*input_var)\n","        loss = criterion(output, target_var)\n","\n","        # Measure accuracy and record loss\n","        if args.task == 'regression': # Loss and MAE recorded\n","            mae_error = mae(prediction = normalizer.denorm(output.data.cpu()), target = target)\n","            losses.update(loss.data.cpu(), target.size(0))\n","            mae_errors.update(mae_error, target.size(0))\n","        else: \n","            accuracy, precision, recall, fscore, auc_score = class_eval(prediction = output.data.cpu(), target = target)\n","            losses.update(loss.data.cpu().item(), target.size(0))\n","            accuracies.update(accuracy, target.size(0))\n","            precisions.update(precision, target.size(0))\n","            recalls.update(recall, target.size(0))\n","            fscores.update(fscore, target.size(0))\n","            auc_scores.update(auc_score, target.size(0))\n","\n","        # Compute gradient and do SGD step\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Measure elapsed time\n","        batch_time.update(time.time() - end)\n","        end = time.time()\n","\n","        if i % args.print_freq == 0:\n","            if args.task == 'regression':\n","                print('Epoch: [{0}][{1}/{2}]\\t'\n","                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n","                      'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n","                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n","                      'MAE {mae_errors.val:.3f} ({mae_errors.avg:.3f})'.format(epoch, i, len(train_loader),\n","                                                                               batch_time = batch_time,\n","                                                                               data_time = data_time,\n","                                                                               loss = losses,\n","                                                                               mae_errors = mae_errors))\n","            else:\n","                print('Epoch: [{0}][{1}/{2}]\\t'\n","                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n","                      'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n","                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n","                      'Accu {accu.val:.3f} ({accu.avg:.3f})\\t'\n","                      'Precision {prec.val:.3f} ({prec.avg:.3f})\\t'\n","                      'Recall {recall.val:.3f} ({recall.avg:.3f})\\t'\n","                      'F1 {f1.val:.3f} ({f1.avg:.3f})\\t'\n","                      'AUC {auc.val:.3f} ({auc.avg:.3f})'.format(epoch, i, len(train_loader),\n","                                                                 batch_time = batch_time,\n","                                                                 data_time = data_time,\n","                                                                 loss = losses,\n","                                                                 accu = accuracies,\n","                                                                 prec = precisions,\n","                                                                 recall = recalls, \n","                                                                 f1 = fscores,\n","                                                                 auc = auc_scores))\n","                "],"metadata":{"id":"pR7w1I-_G-UW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### validate\n","\n"],"metadata":{"id":"we0ejPCQ3u4V"}},{"cell_type":"code","source":["def validate(val_loader, model, criterion, normalizer, test = False):\n","\n","    # We will record batch_time, losses with value and average\n","    batch_time = AverageMeter()\n","    losses = AverageMeter()\n","\n","    if args.task == 'regression':\n","        mae_errors = AverageMeter()\n","    else:\n","        accuracies = AverageMeter() # 정확도\n","        precisions = AverageMeter() # 정밀도\n","        recalls    = AverageMeter() # 재현율\n","        fscores    = AverageMeter() # F1 score\n","        auc_scores = AverageMeter() # AUC\n","\n","    if test: # test data loader를 사용하는 경우\n","        test_targets = []\n","        test_preds = []\n","        test_cif_ids = []\n","\n","    # Switch to evaluate mode\n","    model.eval()\n","\n","    end = time.time()\n","    for i, (input, target, batch_cif_ids) in enumerate(val_loader):\n","        \n","        if args.cuda: # GPU\n","            with torch.no_grad(): # Disable gradient calculation\n","                input_var = (Variable(input[0].cuda(non_blocking = True)),\n","                             Variable(input[1].cuda(non_blocking = True)),\n","                             input[2].cuda(non_blocking = True),\n","                             [crys_idx.cuda(non_blocking = True) for crys_idx in input[3]])\n","        else: # CPU\n","            with torch.no_grad(): # Disable gradient calculation\n","                input_var = (Variable(input[0]),\n","                             Variable(input[1]),\n","                             input[2],\n","                             input[3])\n","        \n","        if args.task == 'regression':\n","            target_normed = normalizer.norm(target) # target value들을 normalize(정규화)\n","        else:\n","            target_normed = target.view(-1).long()  # target(label)들을 1차원 벡터 및 정수데이터로 변환\n","\n","        if args.cuda: # GPU\n","            with torch.no_grad(): # Disable gradient calculation\n","                target_var = Variable(target_normed.cuda(non_blocking = True)) # target value -> cuda device로 넘김.\n","        else:         # CPU\n","            with torch.no_grad(): # Disable gradient calculation\n","                target_var = Variable(target_normed) # target value -> 그대로 CPU device에서 사용\n","\n","        # Compute output\n","        output = model(*input_var)\n","        loss = criterion(output, target_var)\n","\n","        # Measure accuracy and record loss\n","        if args.task == 'regression':\n","            mae_error = mae(normalizer.denorm(output.data.cpu()), target)\n","            losses.update(loss.data.cpu().item(), target.size(0))\n","            mae_errors.update(mae_error, target.size(0))\n","\n","            if test: # test data loader를 사용하는 경우\n","                test_pred = normalizer.denorm(output.data.cpu())\n","                test_target = target\n","                test_preds += test_pred.view(-1).tolist()\n","                test_targets += test_target.view(-1).tolist()\n","                test_cif_ids += batch_cif_ids\n","\n","        else: # classification\n","            accuracy, precision, recall, fscore, auc_score = class_eval(output.data.cpu(), target)\n","            losses.update(loss.data.cpu().item(), target.size(0))\n","            accuracies.update(accuracy, target.size(0))\n","            precisions.update(precision, target.size(0))\n","            recalls.update(recall, target.size(0))\n","            fscores.update(fscore, target.size(0))\n","            auc_scores.update(auc_score, target.size(0))\n","\n","            if test: # test data loader를 사용하는 경우\n","                test_pred = torch.exp(output.data.cpu())\n","                test_target = target\n","                assert test_pred.shape[1] == 2\n","                test_preds += test_pred[:, 1].tolist()\n","                test_targets += test_target.view(-1).tolist()\n","                test_cif_ids += batch_cif_ids\n","        \n","        # Measure elapsed time\n","        batch_time.update(time.time() - end)\n","        end = time.time()\n","\n","        if i % args.print_freq == 0:\n","            if args.task == 'regression':\n","                print('Test : [{0}/{1}]\\t'\n","                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n","                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n","                      'MAE {mae_errors.val:.3f} ({mae_errors.avg:.3f})'.format(i, len(val_loader),\n","                                                                               batch_time = batch_time,\n","                                                                               loss = losses,\n","                                                                               mae_errors = mae_errors))\n","            else:\n","                print('Test : [{0}/{1}]\\t'\n","                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n","                      'Loss {loss.val:.3f} ({loss.avg:.4f})\\t'\n","                      'Accu {accu.val:.3f} ({accu.avg:.3f})\\t'\n","                      'Precision {prec.val:.3f} ({prec.avg:.3f})\\t'\n","                      'Recall {recall.val:.3f} ({recall.avg:.3f})\\t'\n","                      'F1 {f1.val:.3f} ({f1.avg:.3f})\\t'\n","                      'AUC {auc.val:.3f} ({auc.avg:.3f})'.format(i, len(val_loader),\n","                                                                 batch_time = batch_time,\n","                                                                 loss = losses,\n","                                                                 accu = accuracies, \n","                                                                 prec = precisions,\n","                                                                 recall = recalls,\n","                                                                 f1 = fscores,\n","                                                                 auc = auc_scores))\n","                \n","    if test: # Test data loader 사용하는 경우\n","        star_label = '**'\n","        import csv\n","        with open('test_results.csv', 'w') as f: # Test data에 대한 prediction value를 csv file로 저장\n","            writer = csv.writer(f)\n","            for cif_id, target, pred in zip(test_cif_ids, test_targets, test_preds):\n","                writer.writerow((cif_id, target, pred))\n","    else:\n","        star_label = '*'\n","\n","\n","    if args.task == 'regression':\n","        print(' {star} MAE {mae_errors.avg:.3f}'.format(star = star_label, mae_errors = mae_errors))\n","        return mae_errors.avg\n","    else: # classification\n","        print(' {star} AUC {auc.avg:.3f}'.format(star = star_label, auc = auc_scores))\n","        return auc_scores.avg"],"metadata":{"id":"8MyoZ3mbqe5x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### main"],"metadata":{"id":"0QCDBCdr3xwn"}},{"cell_type":"code","source":["def main():\n","    global args, best_mae_error\n","\n","    # Load data\n","    dataset = CIFData(*args.data_options) # CIFData에 root directory(dataset directory) 입력하여 dataset pipeline 준비\n","    collate_fn = collate_pool             # Graph data batch를 처리하는 collate function\n","    train_loader, val_loader, test_loader = get_train_val_test_loader(dataset = dataset,\n","                                                                      collate_fn = collate_fn,\n","                                                                      batch_size = args.batch_size,\n","                                                                      train_ratio = args.train_ratio,\n","                                                                      num_workers = args.workers,\n","                                                                      val_ratio = args.val_ratio,\n","                                                                      test_ratio = args.test_ratio,\n","                                                                      pin_memory = args.cuda,\n","                                                                      train_size = args.train_size,\n","                                                                      val_size = args.val_size,\n","                                                                      test_size = args.test_size,\n","                                                                      return_test = True)\n","    \n","    # Obtain target value normalizer\n","    if args.task == 'classification':\n","        normalizer = Normalizer(torch.zeros(2))\n","        normalizer.load_state_dict({'mean' : 0, 'std' : 1.}) # 평균 0, 표준편차 1로 정규화(Normalize) 예정\n","    else: # regression\n","        if len(dataset) < 500:\n","            warnings.warn('Dataset has less than 500 data points. Lower accuracy is expected.')\n","            sample_data_list = [dataset[i] for i in range(len(dataset))]\n","        else:\n","            sample_data_list = [dataset[i] for i in sample(range(len(dataset)), 500)]\n","        \n","        _, sample_target, _ = collate_pool(sample_data_list) # sample data를 collate function으로 graph batch data 변환\n","        normalizer = Normalizer(sample_target) # sample target value들을 해당 데이터 분포에 따라 Normalize\n","\n","    # Build model\n","    structures, _, _ = dataset[0]               # Crystal structure\n","    orig_atom_fea_len = structures[0].shape[-1] # Original atomic feature length : 92 (by JSON initial file)\n","    nbr_fea_len = structures[1].shape[-1]       # Neighbor feature length\n","    model = CrystalGraphConvNet(orig_atom_fea_len = orig_atom_fea_len,\n","                                nbr_fea_len = nbr_fea_len,\n","                                atom_fea_len = args.atom_fea_len,\n","                                n_conv = args.n_conv,\n","                                h_fea_len = args.h_fea_len,\n","                                n_h = args.n_h,\n","                                classification = True if args.task == 'classification' else False)\n","    if args.cuda:    # GPU \n","        model.cuda() # model -> cuda device\n","\n","    # Define loss func and optimizer\n","    if args.task == 'classification':\n","        criterion = nn.NLLLoss( ) # Negative Log Likelihood Loss\n","    else:\n","        criterion = nn.MSELoss( ) # Mean Squared Error (Squared L2 norm)\n","\n","    if args.optim == 'SGD':\n","        optimizer = optim.SGD(model.parameters(), args.lr,\n","                              momentum = args.momentum,\n","                              weight_decay = args.weight_decay)\n","    elif args.optim == 'Adam':\n","        optimizer = optim.Adam(model.parameters(), args.lr,\n","                               weight_decay = args.weight_decay)\n","    else: # We could use another state-of-the-art optimizer such as [torch.optim.LBFGS]\n","        raise NameError('Only SGD or Adam is allowed as --optim')\n","\n","    # ========= Optionally resume from a checkpoint ==========\n","    if args.resume:\n","        if os.path.isfile(args.resume):\n","            print(\"=> loading checkpoint '{}'\".format(args.resume))\n","            checkpoint = torch.load(args.resume)\n","            args.start_epoch = checkpoint['epoch']\n","            best_mae_error = checkpoint['best_mae_error']\n","            model.load_state_dict(checkpoint['state_dict'])\n","            optimizer.load_state_dict(checkpoint['optimizer'])\n","            normalizer.load_state_dict(checkpoint['normalizer'])\n","            print(\"=> loaded checkpoint '{}' (epoch {})\".format(args.resume, checkpoint['epoch']))\n","        else:\n","            print(\"=> no checkpoint found at '{}'\".format(args.resume))\n","    # ========================================================\n","\n","    # Learning rate scheduler\n","    scheduler = MultiStepLR(optimizer, milestones = args.lr_milestones, gamma = 0.1)\n","\n","    for epoch in range(args.start_epoch, args.epochs):\n","\n","        # Train for one epoch\n","        train(train_loader, model, criterion, optimizer, epoch, normalizer)\n","\n","        # Evaluate on validation set\n","        mae_error = validate(val_loader, model, criterion, normalizer)\n","\n","        # ========== Check whether the mae_error is Nan or not  ==========\n","        if mae_error != mae_error: # Wow, Surprising code!!\n","            print('Exit due to NaN')\n","            sys.exit(1)\n","        # ================================================================\n","\n","        scheduler.step() # Learning rate decayed \n","\n","        # Remember the best mae_error and save checkpoint\n","        if args.task == 'regression':\n","            is_best = mae_error < best_mae_error\n","            best_mae_error = min(mae_error, best_mae_error)\n","        else:\n","            is_best = mae_error > best_mae_error \n","            best_mae_error = max(mae_error, best_mae_error) # Why choose max value of mae_error?? --> check classification loss \n","        \n","        save_checkpoint({'epoch' : epoch + 1,\n","                         'state_dict' : model.state_dict(),\n","                         'best_mae_error' : best_mae_error,\n","                         'optimizer' : optimizer.state_dict(),\n","                         'normalizer' : normalizer.state_dict(),\n","                         'args' : vars(args)},\n","                         is_best)\n","        \n","    # Test best model\n","    print('---------- Evaluate Model on Test Set ----------')\n","    best_checkpoint = torch.load('model_best.pth.tar')\n","    model.load_state_dict(best_checkpoint['state_dict'])\n","    validate(test_loader, model, criterion, normalizer, test = True)\n"],"metadata":{"id":"SWCOR6Le2KOu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if __name__ == '__main__':\n","    main()"],"metadata":{"id":"36XIzlE7fQMh"},"execution_count":null,"outputs":[]}]}